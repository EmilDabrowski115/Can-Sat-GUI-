<doc>
  <assembly>
    <name>ILNumerics.Toolboxes.Optimization</name>
  </assembly>
  <members>
    <member name="T:ILNumerics_Licensing.ILLicHelper_55ef6cd6a14a410aa888a24758dfa8f3_">
      <summary>
            This type supports ILNumerics infrastructure. Do not edit the code!
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <remarks>This type is automatically generated by ILNumerics Ultimate VS. If you encounter problems compiling this file, make sure to reference ILNumerics.Core in your project!<para>[ILNumerics Optimization Toolbox]</para></remarks>
    </member>
    <member name="T:ILNumerics.Toolboxes.OptimizationInternal">
      <summary>
            The ILNumerics Optimization Toolbox is a collection of functions that extend the capability 
            of the ILNumerics computing environment. The toolbox includes routines for solving
            optimization problems including unconstrained and constrained nonlinear optimization as well as least square problems. 
            General documentation is available at: <a href="http://ilnumerics.net/ilnumerics-optimization-toolbox.html">ILNumerics toolbox documentation</a>.
            </summary>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.Beale(ILNumerics.ILInArray{System.Double})">
      <summary>
            Beale function, vectorized
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x">Position to be evaluated, must have only two components</param>
      <returns>The value of the Beale function at the given position</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.Rosenbrock(ILNumerics.ILInArray{System.Single})">
      <summary>
            Rosenbrock function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x"> The position to be evaluated. it can have more than 2 two components</param>
      <returns>The evaluation of the Rosembrock function at the given position</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.Rosenbrock(ILNumerics.ILInArray{System.Double})">
      <summary>
            Rosenbrock function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x"> The position to be evaluated. it can have more than 2 two components</param>
      <returns>The evaluation of the Rosembrock function at the given position</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.Second(ILNumerics.ILInArray{System.Double})">
      <summary>
            Second function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x">Position to be evaluated. The position is in R<sup>n</sup></param>
      <returns>Evaluation of the function at a given point x or columwise evaluation of x if x is a matrix.</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.Camel3(ILNumerics.ILInArray{System.Double})">
      <summary>
            Camel function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x"> Position to be evaluated</param>
      <returns>The value of the Camel function at a given position</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.Griewank(ILNumerics.ILInArray{System.Double})">
      <summary>
            Griewank function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x"> Positin to be evaluated in dimension n </param>
      <returns>An evaluation of the Griewank function at a given position</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.Griewank(ILNumerics.ILInArray{System.Single})">
      <summary>
            Griewank function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x">Position to be evaluated, length n</param>
      <returns>Evaluation of the Griewank function at the given position</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.ToyFunction(ILNumerics.ILInArray{System.Double})">
      <summary>
            Toy function for the toy problem
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="xp"> Position to be evaluated in dimension n</param>
      <returns>An evaluation of the Toy function at a given point xp</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.InequalityConstrainedToyFunction(ILNumerics.ILInArray{System.Double})">
      <summary>
            Inequality constraint for the Toy problem
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x"> The position where the inequality constraint function has to be evaluated</param>
      <returns>The value of the inequality constraint at x </returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.EqualityConstrainedToyFunction(ILNumerics.ILInArray{System.Double})">
      <summary>
            Equality constraint for the toy problem
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x">Position to be evaluated</param>
      <returns>An evaluation of the equality constraint of the toy problem at x</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.polynomialFunction(ILNumerics.ILInArray{System.Double})">
      <summary>
            Sample polynomial function for optimization
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="xp"> Position to be evaluated</param>
      <returns>An evaluation of the function at xp</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.InequalityConstrained(ILNumerics.ILInArray{System.Double})">
      <summary>
            ho: this seems pretty useless! It is completely unclear what it does! ... Dany: A second inequality constraint sample for constraint optimization
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="xp">Position to be evaluated</param>
      <returns>The evaluation of the function at xp</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.DerivN(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},System.Double,System.Int32,System.Int32,System.Double@)">
      <summary>
            Function to find the pointwise derivative useful for the gradient function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="func">the function to be evaluated</param>
      <param name="x">The position</param>
      <param name="step">The optional initial step size</param>
      <param name="i">Abscisse of the index</param>
      <param name="j"></param>
      <param name="internal_Err"></param>
      <returns></returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.DerivAllN(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},System.Double,System.Int32,System.Double@)">
      <summary>
            Compute a column of the jacobian matrix at once
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="func">the function to be evaluated</param>
      <param name="x">The position</param>
      <param name="step">The optional initial step size</param>
      <param name="j"></param>
      <param name="internal_Err"></param>
      <returns>Estimate of the column of the jacobian matrix of func at x</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.Hessian(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},System.Double,System.Int32,System.Int32,System.Double@,System.Double@)">
      <summary>
            program to compute the hessian at one giving point and for a given component, using finite differences
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="func">The function to be evaluated</param>
      <param name="x">The abcisse where the hessian is requested</param>
      <param name="step">The step size for the computation</param>
      <param name="i">Line of the hessian</param>
      <param name="j">Column of the hessian</param>
      <param name="internal_Err"> Internal error during the computation </param>
      <param name="Step">Step used at the end of the Ridders iteration</param>
      <returns>The (i,j) th component of the hessian of func at x</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.newtonCGLineSearch(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},System.Double,System.Double,System.Int32)">
      <summary>
Base class for direction computation into the BGFS algorithm, provides the search direction
<para>[ILNumerics Optimization Toolbox]</para>
</summary>
      <param name="func">func is a function define from R<sup>n</sup> to R</param>
      <param name="xN"> Vector giving the actual position</param>
      <param name="alpha">Bfgs parameter </param>
      <param name="Bk">Bfgs approximation of the hessian</param>
      <param name="gradk">Actual gradient of func</param>
      <param name="maxIter">Maximum number of iteration allowed. Default: maxIter=100 </param>
      <param name="tol"> Tolerance</param>
      <returns> An array of dimension same size as x</returns>
      <remarks>
        <para>If one of the input arrays is empty, an empty array will be returned. This is better for nice functions</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <exception cref="T:System.ArgumentNullException"> If x has a null component</exception>
      <exception cref="T:System.ArgumentOutOfRangeException"> If f is not defined at x or if f has values in R^n, for n&gt;1</exception>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.lineSearchGoldenSection(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},System.Double,System.Double,System.Double,System.Int32,ILNumerics.ILOutArray{System.Int32})">
      <summary>
            Line search algorithm combining the Golden number as used in the bfgs function, to find the minimum of func(xsearchline+apha*pK)
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="func">The imput function</param>
      <param name="xsearchline">The position </param>
      <param name="pK">New computed search direction</param>
      <param name="gradk">Gradient of the function func</param>
      <param name="alpha">Initial step for the line search</param>
      <param name="beta">Coefficient for the Wolfe condition</param>
      <param name="rho">Golden number</param>
      <param name="nb">Maximum number of iterations</param>
      <param name="nFct">Number of the function evaluation</param>
      <returns>a scalar alphak allowing to update the bfgs algorithm in the right direction and minimizing func(xsearchline+apha*pK)</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.HessianBFGS(ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})">
      <summary>
            Approximation of the hessian function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="Bk">The previous approximation of the hessian</param>
      <param name="yk">variation in  the gradient</param>
      <param name="sk">Variation in the between two point in the space</param>
      <returns>A new approximation of the hessian</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.Lagrangian(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double})">
      <summary>
            Lagrangian for constrained optimization
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="function"></param>
      <param name="x"></param>
      <param name="rhok"></param>
      <param name="lambdak"></param>
      <param name="muk"></param>
      <param name="EquConst"></param>
      <param name="IneqConst"></param>
      <returns>The lagrangian of constrained optimization problem</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.StepsizeComputation(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},System.Int32,ILNumerics.ILInArray{System.Double})">
      <summary>
            Computes a good stepsize for the precise jacobian algorithm
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objfunc">objective function</param>
      <param name="ej">index of dimension in x to compute step size for </param>
      <param name="fx">function evaluation at x</param>
      <param name="x">Position vector of objfunc evaluation</param>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.Projection(ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})">
      <summary>
            Projection of a position in an interval for bounded constrained optimization
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x">Position to be projected</param>
      <param name="l">Lower-bound of the interval</param>
      <param name="u">Upper-bound of the interval</param>
      <returns>The projection of x between l and u</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.feval(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double})">
      <summary>
            Evaluation of a function at a given position
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="func">Function to be evaluated</param>
      <param name="x">Position where the function is evaluated</param>
      <returns> An array of dimension numberOfLines(f) X numberOfColumns(x)</returns>
      <remarks>
        <para>If one of the input arrays is empty, an empty array will be returned.</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <example>
        <code><![CDATA[
            ILArray<double> x0 = ILMath.array<double>(new double[] { 1.0, 2.1, 1.0, 3.0, 1.0, 2.1, 1.0, 3.0 }, 4, 2); //Define the input vector;
            Optimization.feval(x => x * x, x0.T);// Compute the evaluation of x<sup>2</sup> at for positions (1.0,2.1), (1.0,3.0), (1.0,2.1) and  (1.0,3.0); 
            ]]></code>
      </example>
      <exception cref="T:System.ArgumentNullException"> If x is null</exception>
      <exception cref="T:System.ArgumentOutOfRangeException"> If x contains inf or NaN</exception>
      <seealso cref="M:ILNumerics.Toolboxes.OptimizationInternal.jacobian_prec(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})" />
      <seealso cref="F:ILNumerics.Toolboxes.Optimization.DefaultDerivativeFunction" />
      <returns> 
            A line vector giving the value of func at x when x is a vector; the column-wise evaluation of x if x is a matrix.
            </returns>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.jacobian_prec(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})">
      <summary>
            High precision numerical estimation of the jacobian matrix of the objective function at the position x
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="func">Objective function(s)</param>
      <param name="x">Vector giving the current position of evaluation</param>
      <param name="Fx">Vector with the result of the evaluation of the functions at <paramref name="x" /></param>
      <remarks>
        <para>This is a precise version of the jacobian estimation, using (forward+backward) finite differences 
            with automatic step size tuning and Ridders' method of polynomial extrapolation.</para>
        <para>If an input array is empty, an empty array will be returned.</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <example>
        <para>
          <code><![CDATA[ 
            ILArray<double> x0 = 10 * rand(10, 1);
            ILArray<double> grad = diag(Optimization.jacobian_prec(x => cos(x), x0, null));
            ]]></code>
        </para>
      </example>
      <exception cref="T:System.ArgumentNullException"> If x has a null component</exception>
      <see cref="M:ILNumerics.Toolboxes.OptimizationInternal.jacobian_fast(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})" />
      <exception cref="T:System.ArgumentOutOfRangeException"> If f is not defined at x</exception>
      <returns> An array of dimension numberOfLines(f) X numberOfColumns(x)</returns>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.jacobian_fast(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})">
      <summary>
            Efficient numerical estimation of the jacobian matrix (forward finite differences).
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="func">Function to be evaluated</param>
      <param name="x">Vector giving the position of current evaluation</param>
      <param name="Fx">[optional] Vector with results of the evaluation of the objective function</param>
      <remarks>
        <para>This algorithm computes a good estimate of the jacobian matrix, concentrating on speed rather than on exactness.</para>
        <para>On empty input x, an empty array will be returned.</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <example>
        <para>Find the minimizer for f(x) = sin(x) + cos(x); x,f in R<sup>3</sup>, start searching at (1,1,1), using levenberg marquardt algorithm. 
            The <see cref="M:ILNumerics.Toolboxes.OptimizationInternal.jacobian_fast(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})" /> jacobian estimate is used for computing the jacobian in each step:
            <code><![CDATA[ 
            ILArray<double> Minimizer = Optimization.optimlevm(x => sin(x) + cos(x), zeros<double>(3, 1), jacobianFunc: Optimization.jacobian_fast);
            ]]></code></para>
      </example>
      <exception cref="T:System.ArgumentNullException"> If x has a null component</exception>
      <exception cref="T:System.ArgumentOutOfRangeException"> If f is not defined at x</exception>
      <see cref="M:ILNumerics.Toolboxes.OptimizationInternal.jacobian_prec(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})" />
      <returns> Jacobian estimate, matrix of size func().Length by x.Length</returns>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.hessian(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})">
      <summary>
            Numerical approximation of the hessian of func at position x through Ridders' method of polynomial extrapolation and a finite differences algorithm
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="func">Scalar function defined from R<sup>n</sup> to R</param>
      <param name="x">Vector giving the position of evaluation in R<sup>n</sup></param>
      <param name="fx">[optional] Result of evaluating <paramref name="func" />  at <paramref name="x" />, default: null</param>
      <remarks>
        <para>If one of the input arrays is empty, an empty array will be returned. 
            This is better for C<sup>2</sup> functions</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <example>
        <code><![CDATA[
            ObjectiveFunction<double> polynom = x => norm(x, 1) + 1;
            ILArray<double> C = Optimization.hessian(polynom, ones<double>(2, 1));]]></code>
      </example>
      <exception cref="T:System.ArgumentNullException"> If x is null</exception>
      <exception cref="T:System.ArgumentOutOfRangeException"> If f is not defined at x or if f has values in R <sup>n</sup>, for n&gt;1</exception>
      <returns> An array of dimension x.Length X x.Length</returns>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.fminunconst_bfgs_newton(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},System.Int32,ILNumerics.Toolboxes.Optimization.DerivativeFunction{System.Double},ILNumerics.Toolboxes.Optimization.DerivativeFunction{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Int32},System.Double,System.Double,ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},System.Double)">
      <summary>
            Find minimizer of a mid-scale unconstrained multivariable nonlinear optimization problem, used internally by fminunconst_bfgs and fminunconst_newton
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objfunc">Cost function to be minimized, defined from R<sup>n</sup> to R</param>
      <param name="x0">Initial solution guess, lenght: (R<sup>n</sup>). Specifies the starting point for the search.</param>
      <param name="maxIter">[optional] Maximal number of iterations allowed. Default: 500</param>
      <param name="gradientFunc">[optional] Function used to compute the gradient. Default: null (finite differences via jacobian_prec)</param>
      <param name="hessianfunc">[optional] if provided, it will replace the BFGS method. (classical newton). Default: null (BFGS) </param>
      <param name="gradientNorm">[optional] Output: Request the gradient norms at each iteration. Default: null (do not compute)</param>
      <param name="iterationCount">[Output] Number of effective iterations during the computation</param>
      <param name="iterations">[optional] Output array of intermediate positions at each iteration. Default: null (not provided)</param>
      <param name="tol">[optional] Exit tolerance on the gradient. Default: 1e-6</param>
      <param name="tolX">[optional] Exit the iterations when the solution is not significantly changing anymore. Default: 1e-6</param>
      <param name="fCost">Additional function for constrained optimization (not to be used directly with bfgs algorithm)</param>
      <param name="epsk">Stopping condition used for constraints optimization algorithm only. Default: 1e-8</param>
      <returns>A local minimizer of func, length: n</returns>
      <remarks>
        <para> If x0 is empty, an empty array of the same size will be returned.</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <seealso href="http://ilnumerics.net/unconstrained-optimization.html" />
      <example>
        <code><![CDATA[
            ILArray<double> x0 = -8 *ILMath.ones(2, 1); // initial guess for the solution 
            ILArray<double> Mini = Optimization.fminunconst_bfgs(SampleFunction, x0); // minimizer of the SampleFuntion in two dimensions; 
            //Sample function definition
            public static ILRetArray<double> SampleFunction(ILInArray<double> A)
            {
               using (ILScope.Enter(A))
               {
                   return 2 * x[0] * x[0] + x[0] * x[1] + 2 * x[1] * x[1] - 6 * x[0] - 6 * x[1] + 15;
               }
            }]]></code>
      </example>
      <exception cref="T:System.ArgumentOutOfRangeException">If objfunc is not defined at <paramref name="x0" /> or if objfunc was found to be non-scalar</exception>
      <exception cref="T:System.ArgumentNullException">If one of <paramref name="x0" /> or <paramref name="objfunc" /> was null on entry</exception>
      <seealso href="http://ilnumerics.net/unconstrained-optimization.html" />
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.fminunconst_lbfgs_int(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},System.Int32,ILNumerics.Toolboxes.Optimization.DerivativeFunction{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Int32},System.Double,System.Double,ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},System.Double)">
      <summary>
            Find minimizer of a large-scale unconstrained multivariable nonlinear optimization problem
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objfunc">Cost function to be minimized, defined from R<sup>n</sup> to R</param>
      <param name="x0">Initial solution guess, lenght: (R<sup>n</sup>). Specifies the starting point for the search.</param>
      <param name="maxIter">[optional] Maximal number of iterations allowed. Default: 500</param>
      <param name="gradientFunc">[optional] Function used to compute the gradient. Default: null (finite differences via jacobian_prec)</param>
      <param name="gradientNorm">[optional] Output: Request the gradient norms at each iteration. Default: null (do not compute)</param>
      <param name="iterationCount">[Output] Number of effective iterations during the computation</param>
      <param name="iterations">[optional] Output array of intermediate positions at each iteration. Default: null (not provided)</param>
      <param name="tol">[optional] Exit tolerance on the gradient. Default: 1e-6</param>
      <param name="tolX">[optional] Exit the iterations when the solution is not significantly changing anymore. Default: 1e-6</param>
      <param name="fCost">Additional function for constrained optimization (not to be used directly with bfgs algorithm)</param>
      <param name="epsk">Stopping condition used for constraints optimization algorithm only. Default: 1e-8</param>
      <returns>A local minimizer of func, length: n</returns>
      <remarks>
        <para> If x0 is empty, an empty array of the same size will be returned.</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <seealso href="http://ilnumerics.net/unconstrained-optimization.html" />
      <example>
        <code><![CDATA[
            ILArray<double> x0 = -8 *ILMath.ones(2, 1); // initial guess for the solution 
            ILArray<double> Mini = Optimization.fminunconst_lbfgs(SampleFunction, x0); // minimizer of the SampleFuntion in two dimensions; 
            //Sample function definition
            public static ILRetArray<double> SampleFunction(ILInArray<double> A)
            {
               using (ILScope.Enter(A))
               {
                   return 2 * x[0] * x[0] + x[0] * x[1] + 2 * x[1] * x[1] - 6 * x[0] - 6 * x[1] + 15;
               }
            }]]></code>
      </example>
      <exception cref="T:System.ArgumentOutOfRangeException">If objfunc is not defined at <paramref name="x0" /> or if objfunc was found to be non-scalar</exception>
      <exception cref="T:System.ArgumentNullException">If one of <paramref name="x0" /> or <paramref name="objfunc" /> was null on entry</exception>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.leastsq_pdl(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.DerivativeFunction{System.Double},System.Int32,ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Int32},ILNumerics.ILOutArray{System.Int32},System.Double,System.Double)">
      <summary>
             Powell's dog leg algorithm for vectorial least squares minimization problem.
             <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objfunc">Vectorial cost function  defined from R<sup>n</sup> to R<sup>m</sup></param>
      <param name="x0">Starting guess for the parameter vector in R<sup>n</sup></param>
      <param name="ydata">[optional] data point vector y in R<sup>m</sup>. Leave empty if y is included in objfunc: min||f(x) - ydata||<sup>2</sup>. Default: null</param>
      <param name="jacobianFunc">[Optional] function to compute the Jacobian matrix of the objective function at a certain point. Default: null - internal finite difference algorithm (gradient_fast). </param>
      <param name="maxIter">[optional] maximum number of iteration steps allowed. Default: 200 </param>
      <param name="gradientNorm">[optional] Output array of gradient function norm at each iteration step. Can be used to visualize the progress of the algorithm after each step. Default: null - not provided</param>
      <param name="tol">[Optional] Exit criterion for the distance of the minimizer to the optimal solution. Default: 1e-8</param>
      <param name="tolf">[Optional] Maximum absolute value allowed for the function value at the solution. Default: 1e-8</param>
      <param name="iterations">[Output] Array of intermediate positions at each iteration. Default: null (do not provide intermediate position)</param>
      <param name="iterationCount">Number of effective iterations. Default: null (nubmer is not tracked)</param>
      <param name="nFct">Number of effective cost function evaluations. Default: null (number not tracked)</param>
      <returns>A vector of the same length as x0 containing the solution of min ||f(x)-ydata||^2</returns>
      <remarks>
        <para>
          <c>leastsq_pdl</c> finds the (local) minimum of a vectorial function of several variables, starting at an initial guess:</para>
        <para>argmin{0.5 * sum(f_i(x)^2) } , where  x = [x_1, ..., x_n],   F(x) = (f_i(x)) in R<sup>m</sup>.</para>
        <para> The function also finds the solution of the minimization problem that can be written in the following form: </para>
        <para>
              min ||f(x)-ydata||^2.
             </para>
        <para>
             The returned value will be of the same size as the initial guess x0 provided by the user. 
             On empty initial guess input, an empty array will be returned. </para>
        <para>
          <b>It is recommended to design the objective 
             function and the start parameter in a way which expects the data in columns!</b>
        </para>
        <list type="bullet">
          <item> leastsq_pdl(objfunc,x0) gives a local minimizer of the objective function using the Powell's dog leg algorithm. 
             objfunc reaches a minimum at a minimizer.
             <example><code><![CDATA[
             public static ILRetArray<double> vect_function(ILInArray<double> x)
            {
                 using (ILScope.Enter(x))
                {
                    //
                    // this callback calculates
                    // f0(x0,x1) = 100*(x0+3)^4,
                    // f1(x0,x1) = (x1-3)^4
                    //
                    ILArray<double> fi = array<double>(x.S);
                    fi[0] = 10 * pow(x[0] + 3, 2);
                    fi[1] = pow(x[1] - 3, 2);
                    return fi;
                }
            }
             //Compute now the minimum of the function.
             ILArray<double> xm = Optimization.leastsq_pdl(vect_function, zeros<double>(2, 1));
             // and the result...
             xm
             ><Double> [2,1]
             >      [0]:    -3.0000 
             >      [1]:     3.0000 
             ]]></code></example></item>
          <item>leastsq_pdl(objfunc,x0,jacobianFunc) gives a local minimizer of the objective function using the Powell's dog leg algorithm with gradient provided. 
             <example><code><![CDATA[
             //Sample function definition
             public static ILRetArray<double> vect_function(ILInArray<double> x)
            {
                 using (ILScope.Enter(x))
                {
                    // f0(x0,x1) = 100*(x0+3)^4,
                    // f1(x0,x1) = (x1-3)^4
                    //
                    ILArray<double> fi = array<double>(x.S);
                    fi[0] = 10 * pow(x[0] + 3, 2);
                    fi[1] = pow(x[1] - 3, 2);
                    return fi;
                }
            }
             // Gradient matrice
              public static ILRetArray<double> function1_jac(ILInArray<double> x, ILInArray<double> F_x)
            {
                using (ILScope.Enter(x))
                {
                    // and Jacobian matrix J = [dfi/dxj]
                    ILArray<double> j = array<double>(x.S);
                    j[0, 0] = 20 * (x[0] + 3);
                    j[0, 1] = 0;
                    j[1, 0] = 0;
                    j[1, 1] = 2 * (x[1] - 3);
                    return j;
                }
            }
             //Compute now the minimum of the function.
             ILArray<double> xm = Optimization.leastsq_pdl(vect_function, zeros<double>(2, 1),jacobianFunc: jac_function);
             // and the result...
             xm
             ><Double> [2,1]
             >      [0]:    -3.0000 
             >      [1]:     3.0000 
            ]]></code></example></item>
          <item>Other combinations of input parameters can be done</item>
        </list>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <exception cref="T:System.ArgumentNullException">If x0 was null</exception>
      <exception cref="T:System.ArgumentOutOfRangeException">If objfunc is not defined at 
             <paramref name="x0" /> or if objfunc was found not to be a scalar function</exception>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.leastsq_levm(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.DerivativeFunction{System.Double},System.Int32,ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Int32},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Int32},System.Double,System.Double)">
      <summary>
            Solves a vectorial least squares problem using the levenberg marquardt algorithm.
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objfunc">Objective function, R<sup>n</sup> -&gt; R<sup>m</sup></param>
      <param name="x0">Initial guess, vector of length n</param>
      <param name="jacobianFunc">[optional] User defined function for computing the jacobian of objfunc at a position x. Default: compute jacobian using fast forward finite differences approach.</param>
      <param name="maxIter">[optional] Maximal number of iterations performed while searching for the solution. Default: 200</param>
      <param name="Fx">[optional] Output: Result of evaluating objfunc at the solution found (residuals). Default: null - not computed</param>
      <param name="iterations">[optional] Output: intermediate solution vectors for each iteration step to the solution. Default: null - not provided</param>
      <param name="iterationCount">[optional] Output: number of iteration steps needed to get to the final solution. Default: null - not returned</param>
      <param name="gradientNorm">[optional] Output array of gradient function norms after each iteration step. Used for convergence verification. Default: null (not computed)</param>
      <param name="nFct">[optional] Output: number of evaluations of the objective function during computation. Default: null - not returned</param>
      <param name="tol">[Optional] Exit criterion for the distance of the minimizer to the optimal solution. Default: 1e-8</param>
      <param name="tolf">[Optional] Maximum error allowed for the function value. Default: 1e-8</param>
      <returns>Solution vector, R<sup>n</sup>. </returns>
      <remarks>optimlev uses the minpack optimization algorithms lmder (jacobian function provided) und lmdif (no jacobian function provided).<para>[ILNumerics Optimization Toolbox]</para></remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.OptimizationInternal.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Double,System.Double,ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})">
      <summary>
             Find minimum of a constrained multivariable optimization problem
             <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objfunc">Cost function to be minimized, defined from R<sup>n</sup> to R</param>
      <param name="x0">Initial solution guess, lenght: (R<sup>n</sup>). Specifies the starting point for the search.</param>
      <param name="lowerBound">Lower bound constraint, vector of length n. Default: double.MinValue (no lower bound)</param>
      <param name="upperBound">Upper bound constraint, vector of length n. Default: double.MaxValue (no upper bound)</param>
      <param name="InequalityConstraint">Function defining the inequality constraints. Default : null (no inequality constraints)</param>
      <param name="EqualityConstraint">Function defining the equality constraints. Default : null (no equality constraints)</param>
      <param name="hessUpdates">[optional] Optimization method to be used to minimize the lagrangian. Default: Bfgs</param>
      <param name="maxIter">[optional] Maximal number of iterations allowed. Default: 500</param>
      <param name="tolX">[optional] Exit condition on the difference between subsequent intermediate solutions. Default: 1e-8</param>
      <param name="tol">[optional] Tolerance on the quality of the solution for the subproblems, measured by the norm of the gradient. Default: 1e-8</param>
      <param name="gradientNorm">[optional] Output: Request the gradient norms of the Lagrangian at each iteration. Default: null (do not compute)</param>
      <param name="iterations">[optional] Output array of intermediate positions at each iteration. Default: null (not provided)</param>
      <param name="callback">Callback function to be called in each iteration. See: <see cref="T:ILNumerics.Toolboxes.FminCallbackInfo`1" /></param>
      <remarks>
        <para>
          <c>fmin</c> finds the minimum of a scalar function of several variables, starting at an initial guess and subject to constraints if provided. </para>
        <para>The minimization problem can be written in the following form:</para>
        <para>
             min objfunc(x)
             </para>
        <para>
             subject to  InequalityConstraint(x) <c> negative </c>, EqualityConstraint(x)=0,  x  <c> between </c> lowerBound and upperBound. 
             </para>
        <para>
             The returned value will be of the same size as the initial guess x0 provided by the user. 
             On empty initial guess input, an empty array will be returned. 
             However, on empty lower or upper bounds, the input will not be taken into account. 
             This is generally referred to as constrained nonlinear optimization
             <list type="bullet"><item> fmin(objfunc,x0) gives a local minimizer of the objective function using the bfgs algorithm. objfunc reaches a minimum at a minimizer.
             <example><code><![CDATA[
             ILArray<double> x0 = -8 *ILMath.ones(4, 1); // initial guess for the computation 
             ILArray<double> Mini = Optimization.fmin(Optimization.Rosenbrock, x0); // minimizer of the Rosembrock function in dimension 4; 
             ]]></code></example></item><item>fmin(objfunc,x0,InequalityConstraint) gives a local minimizer of the objective function using the bfgs algorithm subject to inequality constraints. 
             <example><code><![CDATA[
             //Sample function definition
             public static ILRetArray<double> SampleFunction(ILInArray<double> xp)
            {
                using (ILScope.Enter(xp))
                {
                    double val = (double)(2 * x[ 0] * x[ 0] + x[ 0] * x[ 1] + 2 * x[ 1] * x[ 1] - 6 * x[ 0] - 6 * x[ 1] + 15);
                    return val;
                }
            }
             public static ILRetArray<double> InequalityConstrainedSampleFunction(ILInArray<double> x)
            {
             using (ILScope.Enter(x))
             {
             double val1 = (double)(x[0] + 2 * x[1]);
             double val2 = (double)(4 * x[0]);
             double val3 = (double)(x[1]);
             double val4 = (double)(-2 * x[0] + 2 * x[1]);
             return array<double>(new double[] { val1, val2, val3, val4 }, 1, 4).T;
             }
             }
             ILArray<double> x0 = ILMath.ones(2, 1); // initial guess for the computation 
             ILArray<double> x = Optimization.fmin(SampleFunction, x0, InequalityConstraint: InequalityConstrainedSampleFunction); // minimizer of the Rosembrock function in dimension 4; ]]></code></example></item><item> fmin(objfunc,x0,EqualityConstraint) gives a local minimizer of the objective function using the bfgs algorithm subject to an equality constraints. 
             <example><code><![CDATA[
             public static ILRetArray<double> EqualityConstrainedSampleFunction(ILInArray<double> x)
             {
             using (ILScope.Enter(x))
             {
             double val = (double)(-2 * x[0] + 2 * x[1] + 1);
             return val;
             }
             }
             ILArray<double> x0 = 6 * ones<double>(2, 1); // initial guess for the computation 
             ILArray<double> Mini = ILArray<double> x = Optimization.fmin(SampleFunction, x0, EqualityConstraint: EqualityConstrainedSampleFunction); // minimizer of the Sample function in dimension 2 with equality constraint; 
             ]]></code></example></item><item>fmin(objfunc,x0,inequalityConstraint, EqualityConstraint) gives a local minimizer of the objective function using the bfgs algorithm subject to inequality and equality constraints. 
             <example><code><![CDATA[
             ILArray<double> x0 = 2e+20 * ones<double>(2, 1); // initial guess for the computation 
             ILArray<double> x = Optimization.fmin(SampleFunction, x0, InequalityConstraint: InequalityConstrainedSampleFunction, EqualityConstraint: EqualityConstrainedSampleFunction); // minimizer of the Sample function in dimension 2 with equality constraint only; 
             ]]></code></example></item><item>fmin(objfunc,x0,inequalityConstraint, EqualityConstraint,lowerBound, upperBound) gives a local minimizer of the objective function using the bfgs algorithm subject to inequality, equality, and bounded constraints. 
             <example><code><![CDATA[
             ILArray<double> x0 = 4e+20 * ones<double>(2, 1); // initial guess for the computation 
             ILArray<double> x = Optimization.fmin(SampleFunction, x0, InequalityConstraint: InequalityConstrainedSampleFunction, EqualityConstraint: EqualityConstrainedSampleFunction); // minimizer of the toy function in dimension 2 with equality constraint only; 
             ]]></code></example></item><item>Other combinations can be done</item></list></para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <exception cref="T:System.ArgumentNullException"> If x0 has a null component</exception>
      <exception cref="T:System.ArgumentOutOfRangeException"> If an component of x0, lowerBound, and upperBound is not NaN or if a component of x0 is infinity </exception>
      <returns> Minimizer of the cost function subject to constraints </returns>
      <seealso href="http://ilnumerics.net/constrained-optimization.html" />
    </member>
    <member name="T:ILNumerics.Toolboxes.HessianUpdateMethod">
      <summary>
            List of updating schemes for the Hessian matrix.
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="F:ILNumerics.Toolboxes.HessianUpdateMethod.BFGS">
      <summary>
            Use BFGS method for updating the Hessian approximation.
            </summary>
    </member>
    <member name="F:ILNumerics.Toolboxes.HessianUpdateMethod.FiniteDiff">
      <summary>
            Use a rather precise finite differences approach for updating the Hessian matrix.
            </summary>
    </member>
    <member name="F:ILNumerics.Toolboxes.HessianUpdateMethod.LBFGS">
      <summary>
            Use the Low memory BFGS to update the Hessian matrix.
            </summary>
    </member>
    <member name="M:ILNumerics.Toolboxes.ILLevmar.#ctor(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.DerivativeFunction{System.Double},System.Int32,System.Boolean,System.Double,System.Double)">
      <summary>
            Create a new levenberg marquardt optimizer instance
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objFunc">Objective function</param>
      <param name="x0">initial guess</param>
      <param name="jacobianFunc">[optional] provide a function to compute the jacobian. Default: forward finite differences</param>
      <param name="maxIter">[optional] maximum number of iterations before exiting. Default: 200</param>
      <param name="trackIterations">[optional] flag indicating if intermediate solutions should be kept. Default: false</param>
      <param name="tol">[optional] Exit algorithm if the relative distance of the current solution estimate to the exact solution is at most tol. Default: 1e-8</param>
      <param name="tolf">[optional] Exit algorithm if the relative error in the sum of squares is at least tolf. Default: 1e-8</param>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="T:ILNumerics.Toolboxes.FminCallbackInfo`1">
      <summary>
            This class provides users of <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" /> with information at each iteration step.
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <typeparam name="T">element type, currently always: System.double</typeparam>
      <remarks>
        <para>If user provides a function to <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" /> as callback, the function gets 
            called at each major iteration. An instance of this class is handled over to the function as parameter. It provides
            extensive information about the state of the optimization process, including details of the subproblem solved in that iteration.</para>
        <para>This class gives access to the following information: </para>
        <list type="bullets">
          <item>
            <b>IterCount</b> - upwards counting number of the major iteration. Each time the callback gets called, 
            <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.IterCount" /> is incremented.</item>
          <item>
            <b>XSub</b> - the current intermediate solution of the subproblem solved. This is not a solution of the constrained 
            optimization problem defined. Instead <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" /> describes a solution of the unconstrained subproblem stated by 
            the Lagrangian in each iteration step.</item>
          <item>
            <b>GradL</b> - the gradient of the Lagrangian at position <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" />. Each major iteration in a 
            constrained optimization problem involves the minimization of an <i>unconstrained subproblem</i> (Lagrangian). The minimum of 
            that subproblem is expected to have a gradient of 0. The actual value returned in <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.GradL" /> serves as a hint 
            for the quality of the solution of the current subproblem.</item>
          <item>
            <b>DistX</b> - the difference between the current value of <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" /> and the value from the last iteration. 
            Decreasing values for <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.DistX" /> in most problems indicate progress towards an optimal solution. <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.DistX" />
            gives the 2-norm of the difference.</item>
          <item>
            <b>EqConst</b> - if equalty constraints are provided for <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" /> this parameter gives the 
            evaluation of these constraints at <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" />. The absolute value of <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.EqConst" /> serves as a measure
            for the violation of the equalty constraint at the current intermediate solution <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" />.</item>
          <item>
            <b>IneqConst</b> - if inequalty constraints have been provided for <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" /> this parameter gives the 
            evaluation of these constraints at <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" />. If the value of <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.IneqConst" /> is larger than 0 this 
            serves as a measure for the violation of the constraints at the current intermediate solution <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" />.</item>
          <item>
            <b>FCost</b> - the cost function gives a measure of the penalty introduced by the constraints (bound-, equalty-, and 
            inequalty constraints) to the current solution value. Since <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" /> tries to minimize these violations
            decreasing values of <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.FCost" /> may indicate a progress towards an optimal solution. However, it is normal for 
            an algorithm run to create non constantly decreasing values.</item>
          <item>
            <b>Cancel</b> - flag used to signal the immediate cancellation of the algorithm.</item>
        </list>
        <para>This class implements the IDisposable interface. The lifetime of the class is controlled by <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" /> and 
            spans the duration of the optimization run only. After the execution returned from <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" /> instances of the 
            info class used in any iteration will be disposed and should not get referenced anymore.</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.FminCallbackInfo`1.Dispose">
      <summary>
            Dispose this instance and all the data contained within. This is managed by ILNumerics automatically. Users must not call Dispose() explicitly.
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="P:ILNumerics.Toolboxes.FminCallbackInfo`1.Cancel">
      <summary>
            Gets a flag indicating whether the iterations should be immediately canceled or sets its value.
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub">
      <summary>
            The current intermediate solution (minimizer) of the subproblem solved in this iteration step [readonly].
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <remarks>This value is not a solution of the constrained optimization problem defined. Instead 
            XSub describes a solution of the unconstrained subproblem stated by 
            the Lagrangian in each iteration step.<para>[ILNumerics Optimization Toolbox]</para></remarks>
    </member>
    <member name="P:ILNumerics.Toolboxes.FminCallbackInfo`1.GradL">
      <summary>
            The gradient of the Lagrangian at position <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" /> [readonly].
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <remarks>Each major iteration in a 
            constrained optimization problem involves the minimization of an <i>unconstrained subproblem</i> (Lagrangian). The minimum of 
            that subproblem is expected to have a gradient of 0. The actual value returned in <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.GradL" /> serves as a hint 
            for the quality of the solution of the current subproblem.<para>[ILNumerics Optimization Toolbox]</para></remarks>
    </member>
    <member name="P:ILNumerics.Toolboxes.FminCallbackInfo`1.DistX">
      <summary>
            The difference between the current value of <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" /> and the value from the last iteration [readonly].
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <remarks>Decreasing values for <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.DistX" /> in most situations indicate progress towards an optimal solution. <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.DistX" />
            gives the 2-norm of the difference.<para>[ILNumerics Optimization Toolbox]</para></remarks>
    </member>
    <member name="P:ILNumerics.Toolboxes.FminCallbackInfo`1.EqConst">
      <summary>
            Evaluation of the equalty constraints (if provided to <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" /> at <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" /> [readonly].
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <remarks>The absolute value of components of <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.EqConst" /> serves as a measure
            for the violation of the equalty constraints at the current intermediate solution <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" />.<para>[ILNumerics Optimization Toolbox]</para></remarks>
    </member>
    <member name="P:ILNumerics.Toolboxes.FminCallbackInfo`1.IneqConst">
      <summary>
            Evaluation of the inequalty constraints (if provided to <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" /> at <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" /> [readonly].
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <remarks>If components of <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.IneqConst" /> are larger than 0 this serves as a measure 
            for the violation of the constraints at the current intermediate solution <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" />.<para>[ILNumerics Optimization Toolbox]</para></remarks>
    </member>
    <member name="P:ILNumerics.Toolboxes.FminCallbackInfo`1.FCost">
      <summary>
            The cost function value gives a measure of the penalty introduced by the constraints [readonly].
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <remarks>This property gives the value obtained by evaluating the cost function at <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.XSub" />. Since 
            <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" /> tries to minimize these violations decreasing values of <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.FCost" /> 
            may indicate a progress towards an optimal solution. However, it is normal for 
            an algorithm run to create non constantly decreasing values.<para>[ILNumerics Optimization Toolbox]</para></remarks>
    </member>
    <member name="P:ILNumerics.Toolboxes.FminCallbackInfo`1.IterCount">
      <summary>
            Upwards counting number of this iteration.
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <remarks>Each time the callback gets called <see cref="P:ILNumerics.Toolboxes.FminCallbackInfo`1.IterCount" /> is incremented.<para>[ILNumerics Optimization Toolbox]</para></remarks>
    </member>
    <member name="T:ILNumerics.Toolboxes.Optimization">
      <summary>
            Optimization is a collection of functions which extend the capability 
            of the ILNumerics Computing Engine. The toolbox includes methods for solving
            optimization problems including unconstrained and constrained nonlinear problems as well as least square problems. 
            General documentation is available online at: <a href="http://ilnumerics.net/ilnumerics-optimization-toolbox.html">ILNumerics toolbox documentation</a>.
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <threadsafety static="true" instance="false" />
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.#cctor">
      <summary>
            License validation for Optimization Toolbox.
            </summary>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="F:ILNumerics.Toolboxes.Optimization.DefaultDerivativeFunction">
      <summary>
            Default derivative function used to compute the Jacobian matrix if no custom implementation is provided. Default: <see cref="M:ILNumerics.Toolboxes.Optimization.jacobian_prec(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})" />.
            </summary>
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.jacobian_prec(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})">
      <summary>
            High precision numerical estimation of the jacobian matrix of the objective function at the position x
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="func">Objective function(s)</param>
      <param name="x">Vector giving the current position of evaluation</param>
      <param name="Fx">Vector with the result of the evaluation of the functions at <paramref name="x" /></param>
      <remarks>
        <para>This is a precise version of the jacobian estimation, using (forward+backward) finite differences 
            with automatic step size tuning and Ridders' method of polynomial extrapolation.</para>
        <para>If an input array is empty, an empty array will be returned.</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <example>
        <para>
          <code><![CDATA[ 
            ILArray<double> x0 = 10 * rand(10, 1);
            ILArray<double> grad = diag(Optimization.jacobian_prec(x => cos(x), x0, null));
            ]]></code>
        </para>
      </example>
      <exception cref="T:System.ArgumentNullException"> If x has a null component</exception>
      <see cref="M:ILNumerics.Toolboxes.Optimization.jacobian_fast(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})" />
      <exception cref="T:System.ArgumentOutOfRangeException"> If f is not defined at x</exception>
      <returns> An array of dimension numberOfLines(f) X numberOfColumns(x)</returns>
      <seealso href="http://ilnumerics.net/ilnumerics-optimization-toolbox.html" />
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.jacobian_fast(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})">
      <summary>
            Efficient numerical estimation of the jacobian matrix (forward finite differences).
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="func">Function to be evaluated</param>
      <param name="x">Vector giving the position of current evaluation</param>
      <param name="Fx">[optional] Vector with results of the evaluation of the objective function</param>
      <remarks>
        <para>This algorithm computes a good estimate of the jacobian matrix, concentrating on speed rather than on exactness.</para>
        <para>On empty input x, an empty array will be returned.</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <example>
        <para>Find the minimizer for f(x) = sin(x) + cos(x); x,f in R<sup>3</sup>, start searching at (1,1,1), using levenberg marquardt algorithm. 
            The <see cref="M:ILNumerics.Toolboxes.Optimization.jacobian_fast(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})" /> jacobian estimate is used for computing the jacobian in each step:
            <code><![CDATA[ 
            ILArray<double> Minimizer = Optimization.optimlevm(x => sin(x) + cos(x), zeros<double>(3, 1), jacobianFunc: Optimization.jacobian_fast);
            ]]></code></para>
      </example>
      <exception cref="T:System.ArgumentNullException"> If x has a null component</exception>
      <exception cref="T:System.ArgumentOutOfRangeException"> If f is not defined at x</exception>
      <see cref="M:ILNumerics.Toolboxes.Optimization.jacobian_prec(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})" />
      <returns> Jacobian estimate, matrix of size func().Length by x.Length</returns>
      <seealso href="http://ilnumerics.net/ilnumerics-optimization-toolbox.html" />
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.hessian(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})">
      <summary>
            Numerical approximation of the hessian of func at position x through Ridders' method of polynomial extrapolation and a finite differences algorithm
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="func">Scalar function defined from R<sup>n</sup> to R</param>
      <param name="x">Vector giving the position of evaluation in R<sup>n</sup></param>
      <param name="fx">[optional] Result of evaluating <paramref name="func" />  at <paramref name="x" />, default: null</param>
      <remarks>
        <para>If one of the input arrays is empty, an empty array will be returned. 
            This is better for C<sup>2</sup> functions</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <example>
        <code><![CDATA[
            ObjectiveFunction<double> polynom = x => norm(x, 1) + 1;
            ILArray<double> C = Optimization.hessian(polynom, ones<double>(2, 1));]]></code>
      </example>
      <exception cref="T:System.ArgumentNullException"> If x is null</exception>
      <exception cref="T:System.ArgumentOutOfRangeException"> If f is not defined at x or if f has values in R <sup>n</sup>, for n&gt;1</exception>
      <returns> An array of dimension x.Length X x.Length</returns>
      <seealso href="http://ilnumerics.net/ilnumerics-optimization-toolbox.html" />
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.fminunconst_bfgs(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},System.Int32,ILNumerics.Toolboxes.Optimization.DerivativeFunction{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Int32},System.Nullable{System.Double},System.Nullable{System.Double})">
      <summary>
            Find minimizer of a medium-scale unconstrained multivariable nonlinear optimization problem, using BFGS updates to the hessian matrix
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objfunc">Cost function to be minimized, defined from R<sup>n</sup> to R</param>
      <param name="x0">Initial solution guess, length: (R<sup>n</sup>). Specifies the starting point for the search.</param>
      <param name="maxIter">[optional] Maximal number of iterations allowed. Default: 500</param>
      <param name="gradientFunc">[optional] Function used to compute the gradient. Default: null (finite differences via <see cref="M:ILNumerics.Toolboxes.Optimization.jacobian_prec(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})" />)</param>
      <param name="gradientNorm">[optional] Output: Request the gradient norms at each iteration. Default: null (do not compute)</param>
      <param name="iterationCount">[Output] Number of effective iterations during the computation</param>
      <param name="iterations">[optional] Output array of intermediate positions at each iteration. Default: null (not provided)</param>
      <param name="tol">[optional] Exit tolerance on the gradient. Default: <see cref="P:ILNumerics.Toolboxes.Optimization.DefaultTolerance" /> (1e-8)</param>
      <param name="tolX">[optional] Exit the iterations when the solution is not significantly changing anymore. Default: <see cref="P:ILNumerics.Toolboxes.Optimization.DefaultTolerance" /> (1e-8)</param>
      <returns>A local minimizer of <paramref name="objfunc" />, length: n</returns>
      <remarks>
        <para> If x0 is empty, an empty array of the same size will be returned.</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <seealso href="http://ilnumerics.net/unconstrained-optimization.html">Unconstrained optimization documentation</seealso>
      <example>
        <code><![CDATA[
            ILArray<double> x0 = -8 *ILMath.ones(2, 1); // initial guess for the solution 
            ILArray<double> Mini = Optimization.fminunconst_bfgs(SampleFunction, x0); // minimizer of the SampleFuntion in two dimensions; 
            //Sample function definition
            public static ILRetArray<double> SampleFunction(ILInArray<double> A)
            {
               using (ILScope.Enter(A))
               {
                   return 2 * x[0] * x[0] + x[0] * x[1] + 2 * x[1] * x[1] - 6 * x[0] - 6 * x[1] + 15;
               }
            }]]></code>
      </example>
      <exception cref="T:System.ArgumentOutOfRangeException">If objfunc is not defined at <paramref name="x0" /> or if objfunc was found to be non-scalar</exception>
      <exception cref="T:System.ArgumentNullException">If one of <paramref name="x0" /> or <paramref name="objfunc" /> was null on entry</exception>
      <seealso href="http://ilnumerics.net/unconstrained-optimization.html" />
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.fminunconst_newton(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.DerivativeFunction{System.Double},System.Int32,ILNumerics.Toolboxes.Optimization.DerivativeFunction{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Int32},System.Nullable{System.Double},System.Nullable{System.Double})">
      <summary>
            Find minimizer of a medum-scale unconstrained multivariable nonlinear optimization problem using a classical newton method with explicit hessian evaluation
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objfunc">Cost function to be minimized, defined from R<sup>n</sup> to R</param>
      <param name="x0">Initial solution guess, lenght: (R<sup>n</sup>). Specifies the starting point for the search.</param>
      <param name="hessianfunc">[optional] Custom function for computing the hessian matrix of <paramref name="objfunc" />. Default: null <see cref="M:ILNumerics.Toolboxes.Optimization.hessian(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})"></see>)</param>
      <param name="maxIter">[optional] Maximal number of iterations allowed. Default: 500</param>
      <param name="gradientFunc">[optional] Function used to compute the gradient. Default: null (finite differences via jacobian_prec)</param>
      <param name="gradientNorm">[optional] Output: Request the gradient norms at each iteration. Default: null (do not compute)</param>
      <param name="iterationCount">[Output] Number of effective iterations during the computation</param>
      <param name="iterations">[optional] Output array of intermediate positions at each iteration. Default: null (not provided)</param>
      <param name="tol">[optional] Exit tolerance on the gradient. Default: <see cref="P:ILNumerics.Toolboxes.Optimization.DefaultTolerance" /> (1e-8)</param>
      <param name="tolX">[optional] Exit the iterations when the solution is not significantly changing anymore. Default: <see cref="P:ILNumerics.Toolboxes.Optimization.DefaultTolerance" /> (1e-8)</param>
      <returns>A local minimizer of func, length: n</returns>
      <remarks>
        <para> If x0 is empty, an empty array of the same size will be returned.</para>
        <para>This function computes the local minimum of the scalar multivariable non-linear optimization problem by explicitly 
            evaluating the hessian matrix. If no hessian function is provided by the user, the predefined <see cref="M:ILNumerics.Toolboxes.Optimization.hessian(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double})" /> 
            function evaluation will be used (finite differences). This leads to very precise results but is rather expensive 
            for demanding problems. In order to control the hessian computation, user can provide a custom hessian function as 
            <see cref="T:ILNumerics.Toolboxes.Optimization.DerivativeFunction`1"></see>.</para>
        <seealso href="http://ilnumerics.net/unconstrained-optimization.html" />
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <example>
        <code><![CDATA[
            ILArray<double> x0 = -8 *ILMath.ones(2, 1); // initial guess for the solution 
            ILArray<double> Mini = Optimization.fminunconst_newton(SampleFunction, x0, hessianfunc: Optimization.hessian); // minimizer of the SampleFuntion in two dimensions; 
            // the hessian argument provided above corresponds to the defaul argument. 
            
            //Sample function definition
            public static ILRetArray<double> SampleFunction(ILInArray<double> A)
            {
               using (ILScope.Enter(A))
               {
                   return 2 * x[0] * x[0] + x[0] * x[1] + 2 * x[1] * x[1] - 6 * x[0] - 6 * x[1] + 15;
               }
            }]]></code>
      </example>
      <exception cref="T:System.ArgumentOutOfRangeException">If objfunc is not defined at <paramref name="x0" /> or if objfunc was found to be non-scalar</exception>
      <exception cref="T:System.ArgumentNullException">If one of <paramref name="x0" /> or <paramref name="objfunc" /> was null on entry</exception>
      <seealso href="http://ilnumerics.net/unconstrained-optimization.html" />
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.fminunconst_lbfgs(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},System.Int32,ILNumerics.Toolboxes.Optimization.DerivativeFunction{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Int32},System.Nullable{System.Double},System.Nullable{System.Double})">
      <summary>
            Find minimizer of a large-scale unconstrained multivariable nonlinear optimization problem
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objfunc">Cost function to be minimized, defined from R<sup>n</sup> to R</param>
      <param name="x0">Initial solution guess, lenght: (R<sup>n</sup>). Specifies the starting point for the search.</param>
      <param name="maxIter">[optional] Maximal number of iterations allowed. Default: 500</param>
      <param name="gradientFunc">[optional] Function used to compute the gradient. Default: null (finite differences via jacobian_prec)</param>
      <param name="gradientNorm">[optional] Output: Request the gradient norms at each iteration. Default: null (do not compute)</param>
      <param name="iterationCount">[Output] Number of effective iterations during the computation</param>
      <param name="iterations">[optional] Output array of intermediate positions at each iteration. Default: null (not provided)</param>
      <param name="tol">[optional] Exit tolerance on the gradient. Default: <see cref="P:ILNumerics.Toolboxes.Optimization.DefaultTolerance" /> (1e-8)</param>
      <param name="tolX">[optional] Exit the iterations when the solution is not significantly changing anymore. Default: <see cref="P:ILNumerics.Toolboxes.Optimization.DefaultTolerance" /> (1e-8)</param>
      <returns>A local minimizer of func, length: n</returns>
      <remarks>
        <para>This functino finds a minimizer of a large-scale unconstrained multivariable nonlinear 
            optimization problem without evaluating the hessian matrix explicitely. For large scale problems with 
            a large number of variables and commonly sparse derivatives, an advantageous performance scheme is achieved which allows the 
            minimizer to be found efficiently without the need of the computation of all hessian elements explicitly.</para>
        <para> If x0 is empty, an empty array of the same size will be returned.</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <seealso href="http://ilnumerics.net/unconstrained-optimization.html" />
      <example>
        <code><![CDATA[
            ILArray<double> x0 = -8 *ILMath.ones(2, 1); // initial guess for the solution 
            ILArray<double> Mini = Optimization.fminunconst_lbfgs(SampleFunction, x0); // minimizer of the SampleFuntion in two dimensions; 
            //Sample function definition
            public static ILRetArray<double> SampleFunction(ILInArray<double> A)
            {
               using (ILScope.Enter(A))
               {
                   return 2 * x[0] * x[0] + x[0] * x[1] + 2 * x[1] * x[1] - 6 * x[0] - 6 * x[1] + 15;
               }
            }]]></code>
      </example>
      <exception cref="T:System.ArgumentOutOfRangeException">If objfunc is not defined at <paramref name="x0" /> or if objfunc was found to be non-scalar</exception>
      <exception cref="T:System.ArgumentNullException">If one of <paramref name="x0" /> or <paramref name="objfunc" /> was null on entry</exception>
      <seealso href="http://ilnumerics.net/unconstrained-optimization.html" />
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})">
      <summary>
             Find minimum of a constrained multivariable optimization problem
             <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objfunc">Cost function to be minimized, defined from R<sup>n</sup> to R</param>
      <param name="x0">Initial solution guess, lenght: (R<sup>n</sup>). Specifies the starting point for the search.</param>
      <param name="lowerBound">Lower bound constraint, vector of length n. Default: double.MinValue (no lower bound)</param>
      <param name="upperBound">Upper bound constraint, vector of length n. Default: double.MaxValue (no upper bound)</param>
      <param name="InequalityConstraint">Function defining the inequality constraints. Default : null (no inequality constraints)</param>
      <param name="EqualityConstraint">Function defining the equality constraints. Default : null (no equality constraints)</param>
      <param name="hessUpdates">[optional] Optimization method to be used to minimize the lagrangian. Default: Bfgs</param>
      <param name="maxIter">[optional] Maximal number of iterations allowed. Default: 500</param>
      <param name="tol">[optional] Tolerance on the quality of the solution for the subproblems, measured by the norm of the gradient. Default: 1e-8</param>
      <param name="tolX">[optional] Exit condition on the difference between subsequent intermediate solutions. Default: DefaultTolerance</param>
      <param name="gradientNorm">[optional] Output: Request the gradient norms of the Lagrangian at each iteration. Default: null (do not compute)</param>
      <param name="iterations">[optional] Output array of intermediate positions at each iteration. Default: null (not provided)</param>
      <param name="callback">Callback function to be called in each iteration. See: <see cref="T:ILNumerics.Toolboxes.FminCallbackInfo`1" /></param>
      <returns> Minimizer of the cost function subject to constraints </returns>
      <remarks>
        <para>
          <c>fmin</c> finds the minimum of a scalar function of several variables, starting at an initial guess and subject to constraints 
             if provided. </para>
        <para>The minimization problem can be written in the following form:</para>
        <para>
             min objfunc(x)
             </para>
        <para>
             subject to <c> InequalityConstraint(x) &lt; 0</c>, <c>EqualityConstraint(x)=0</c>, <c> lowerBound &lt;= x &lt;= upperBound</c>. 
             </para>
        <para>
             The returned value will be of the same size as the initial guess <c>x0</c> provided by the user. 
             <para>On empty initial guess input, an empty array will be returned. </para>
             Lower and upper bounds which are not provided are taken as <c>double.MinValue</c> and <c>double.MaxValue</c> respectively.
             <para>This is generally referred to as <a href="http://ilnumerics.net/constrained-optimization.html">constrained nonlinear optimization</a></para><list type="bullet"><item> fmin(objfunc,x0) gives a local minimizer of the objective function using the bfgs algorithm. objfunc reaches a minimum at a minimizer.
             <example><code><![CDATA[
             ILArray<double> x0 = -8 *ILMath.ones(4, 1); // initial guess for the computation 
             ILArray<double> Mini = Optimization.fmin(Optimization.Rosenbrock, x0); // minimizer of the Rosembrock function in dimension 4; 
             ]]></code></example></item><item>fmin(objfunc,x0,InequalityConstraint) gives a local minimizer of the objective function using the bfgs algorithm subject to inequality constraints. 
             <example><code><![CDATA[
             //Sample function definition
             public static ILRetArray<double> SampleFunction(ILInArray<double> xp)
            {
                using (ILScope.Enter(xp))
                {
                    double val = (double)(2 * x[ 0] * x[ 0] + x[ 0] * x[ 1] + 2 * x[ 1] * x[ 1] - 6 * x[ 0] - 6 * x[ 1] + 15);
                    return val;
                }
            }
             public static ILRetArray<double> InequalityConstrainedSampleFunction(ILInArray<double> x)
            {
             using (ILScope.Enter(x))
             {
             double val1 = (double)(x[0] + 2 * x[1]);
             double val2 = (double)(4 * x[0]);
             double val3 = (double)(x[1]);
             double val4 = (double)(-2 * x[0] + 2 * x[1]);
             return array<double>(new double[] { val1, val2, val3, val4 }, 1, 4).T;
             }
             }
             ILArray<double> x0 = ILMath.ones(2, 1); // initial guess for the computation 
             ILArray<double> x = Optimization.fmin(SampleFunction, x0, InequalityConstraint: InequalityConstrainedSampleFunction); // minimizer of the Rosembrock function in dimension 4; ]]></code></example></item><item>Optimization.fmin(objfunc,x0,EqualityConstraint) gives a local minimizer of the objective function using the bfgs algorithm subject to an equality constraints. 
             <example><code><![CDATA[
             public static ILRetArray<double> EqualityConstrainedSampleFunction(ILInArray<double> x)
             {
             using (ILScope.Enter(x))
             {
             double val = (double)(-2 * x[0] + 2 * x[1] + 1);
             return val;
             }
             }
             ILArray<double> x0 = 6 * ones<double>(2, 1); // initial guess for the computation 
             ILArray<double> Mini = ILArray<double> x = Optimization.fmin(SampleFunction, x0, EqualityConstraint: EqualityConstrainedSampleFunction); // minimizer of the Sample function in dimension 2 with equality constraint; 
             ]]></code></example></item><item>Optimization.fmin(objfunc,x0,inequalityConstraint, EqualityConstraint) gives a local minimizer of the objective function using the bfgs algorithm subject to inequality and equality constraints. 
             <example><code><![CDATA[
             ILArray<double> x0 = 2e+20 * ones<double>(2, 1); // initial guess for the computation 
             ILArray<double> x = Optimization.fmin(SampleFunction, x0, InequalityConstraint: InequalityConstrainedSampleFunction, EqualityConstraint: EqualityConstrainedSampleFunction); // minimizer of the Sample function in dimension 2 with equality constraint only; 
             ]]></code></example></item><item>Optimization.fmin(objfunc,x0,inequalityConstraint, EqualityConstraint,lowerBound, upperBound) gives a local minimizer of the objective function using the bfgs algorithm subject to inequality, equality, and bounded constraints. 
             <example><code><![CDATA[
             ILArray<double> x0 = 4e+20 * ones<double>(2, 1); // initial guess for the computation 
             ILArray<double> x = Optimization.fmin(SampleFunction, x0, InequalityConstraint: InequalityConstrainedSampleFunction, EqualityConstraint: EqualityConstrainedSampleFunction); // minimizer of the toy function in dimension 2 with equality constraint only; 
             ]]></code></example></item><item>All other combinations of input parameters are - of course - allowed also.</item></list></para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <seealso cref="T:ILNumerics.Toolboxes.FminCallbackInfo`1" />
      <exception cref="T:System.ArgumentNullException"> If x0 is null</exception>
      <exception cref="T:System.ArgumentOutOfRangeException"> If an component of x0, lowerBound, and upperBound is no-a-number (NaN) or if a component of x0 is infinity</exception>
      <seealso href="http://ilnumerics.net/constrained-optimization.html" />
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.leastsq_pdl(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.DerivativeFunction{System.Double},System.Int32,ILNumerics.ILInArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Int32},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Int32},System.Double,System.Double)">
      <summary>
             Powell's dog leg algorithm for vectorial least squares minimization problem.
             <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objfunc">Vectorial cost function  defined from R<sup>n</sup> to R<sup>m</sup></param>
      <param name="x0">Starting guess for the parameter vector in R<sup>n</sup></param>
      <param name="jacobianFunc">[Optional] function to compute the Jacobian matrix of the objective function at a certain point. Default: null - internal finite difference algorithm (gradient_fast). </param>
      <param name="maxIter">[optional] maximum number of iteration steps allowed. Default: 200 </param>
      <param name="ydata">[optional] data point vector y in R<sup>m</sup>. Leave empty if y is included in objfunc: min||f(x) - ydata||<sup>2</sup>. Default: null</param>
      <param name="iterationCount">Number of effective iterations. Default: null (nubmer is not tracked)</param>
      <param name="tol">[Optional] Exit criterion for the distance of the minimizer to the optimal solution. Default: 1e-8</param>
      <param name="tolf">[Optional] Maximum absolute value allowed for the function value at the solution. Default: 1e-8</param>
      <param name="iterations">[optional] Output array of intermediate positions at each iteration. Default: null (not provided)</param>
      <param name="gradientNorm">[optional] Output array of gradient function norms after each iteration step. Used for convergence verification. Default: null (not computed)</param>
      <param name="nFct">Number of effective cost function evaluations. Default: null (number not tracked)</param>
      <returns>A vector of the same length as x0 containing the solution of min ||f(x)-ydata||^2</returns>
      <remarks>
        <para>
          <c>leastsq_pdl</c> finds the (local) minimum of a vectorial function of several variables, starting at an initial guess:</para>
        <para>argmin{0.5 * sum(f_i(x)^2) } , where  x = [x_1, ..., x_n],   F(x) = (f_i(x)) in R<sup>m</sup>.</para>
        <para> The function also finds the solution of the minimization problem that can be written in the following form: </para>
        <para>
              min ||f(x)-ydata||^2.
             </para>
        <para>
             The returned value will be of the same size as the initial guess x0 provided by the user. 
             On empty initial guess input, an empty array will be returned. </para>
        <para>
          <b>It is recommended to design the objective 
             function and the start parameter in a way which expects the data in columns!</b>
        </para>
        <list type="bullet">
          <item> leastsq_pdl(objfunc,x0) gives a local minimizer of the objective function using the Powell's dog leg algorithm. 
             objfunc reaches a minimum at a minimizer.
             <example><code><![CDATA[
             public static ILRetArray<double> vect_function(ILInArray<double> x)
            {
                 using (ILScope.Enter(x))
                {
                    //
                    // this callback calculates
                    // f0(x0,x1) = 100*(x0+3)^4,
                    // f1(x0,x1) = (x1-3)^4
                    //
                    ILArray<double> fi = array<double>(x.S);
                    fi[0] = 10 * pow(x[0] + 3, 2);
                    fi[1] = pow(x[1] - 3, 2);
                    return fi;
                }
            }
             //Compute now the minimum of the function.
             ILArray<double> xm = Optimization.leastsq_pdl(vect_function, zeros<double>(2, 1));
             // and the result...
             xm
             ><Double> [2,1]
             >      [0]:    -3.0000 
             >      [1]:     3.0000 
             ]]></code></example></item>
          <item>leastsq_pdl(objfunc,x0,jacobianFunc) gives a local minimizer of the objective function using the Powell's dog leg algorithm with gradient provided. 
             <example><code><![CDATA[
             //Sample function definition
             public static ILRetArray<double> vect_function(ILInArray<double> x)
            {
                 using (ILScope.Enter(x))
                {
                    // f0(x0,x1) = 100*(x0+3)^4,
                    // f1(x0,x1) = (x1-3)^4
                    //
                    ILArray<double> fi = array<double>(x.S);
                    fi[0] = 10 * pow(x[0] + 3, 2);
                    fi[1] = pow(x[1] - 3, 2);
                    return fi;
                }
            }
             // Gradient matrice
              public static ILRetArray<double> function1_jac(ILInArray<double> x, ILInArray<double> F_x)
            {
                using (ILScope.Enter(x))
                {
                    // and Jacobian matrix J = [dfi/dxj]
                    ILArray<double> j = array<double>(x.S);
                    j[0, 0] = 20 * (x[0] + 3);
                    j[0, 1] = 0;
                    j[1, 0] = 0;
                    j[1, 1] = 2 * (x[1] - 3);
                    return j;
                }
            }
             //Compute now the minimum of the function.
             ILArray<double> xm = Optimization.leastsq_pdl(vect_function, zeros<double>(2, 1),jacobianFunc: jac_function);
             // and the result...
             xm
             ><Double> [2,1]
             >      [0]:    -3.0000 
             >      [1]:     3.0000 
            ]]></code></example></item>
          <item>Other combinations of input parameters can be done</item>
        </list>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <exception cref="T:System.ArgumentNullException">If x0 was null</exception>
      <exception cref="T:System.ArgumentOutOfRangeException">If objfunc is not defined at 
             <paramref name="x0" /> or if objfunc was found not to be a scalar function</exception>
      <seealso href="http://ilnumerics.net/ilnumerics-optimization-toolbox.html" />
      <seealso href="http://ilnumerics.net/nonlinear-least-squares.html" />
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.leastsq_levm(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.DerivativeFunction{System.Double},System.Int32,ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Int32},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Int32},System.Double,System.Double)">
      <summary>
            Solves a vectorial least squares problem using the levenberg marquardt algorithm.
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="objfunc">Objective function, R<sup>n</sup> -&gt; R<sup>m</sup></param>
      <param name="x0">Initial guess, vector of length n</param>
      <param name="jacobianFunc">[optional] User defined function for computing the jacobian of objfunc at a position x. Default: compute jacobian using fast forward finite differences approach.</param>
      <param name="maxIter">[optional] Maximal number of iterations performed while searching for the solution. Default: 200</param>
      <param name="Fx">[optional] Output: Result of evaluating objfunc at the solution found (residuals). Default: null - not computed</param>
      <param name="iterations">[optional] Output: intermediate solution vectors for each iteration step to the solution. Default: null - not provided</param>
      <param name="iterationCount">[optional] Output: number of iteration steps needed to get to the final solution. Default: null - not returned</param>
      <param name="gradientNorm">[optional] Output array of gradient function norms after each iteration step. Used for convergence verification. Default: null (not computed)</param>
      <param name="nFct">[optional] Output: number of evaluations of the objective function during computation. Default: null - not returned</param>
      <param name="tol">[Optional] Exit criterion for the distance of the minimizer to the optimal solution. Default: 1e-8</param>
      <param name="tolf">[Optional] Maximum absolute value allowed for the function value at the solution. Default: 1e-8</param>
      <returns>Solution vector, R<sup>n</sup>. </returns>
      <remarks>leastsq_levm uses the minpack optimization algorithms lmder (jacobian function provided) und lmdif (no jacobian function provided).<para>[ILNumerics Optimization Toolbox]</para></remarks>
      <seealso href="http://ilnumerics.net/ilnumerics-optimization-toolbox.html" />
      <seealso href="http://ilnumerics.net/nonlinear-least-squares.html" />
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.Beale(ILNumerics.ILInArray{System.Double})">
      <summary>
            Beale function, 2D, vectorized
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x">Position(s) in R^2, data points in rows</param>
      <returns>The value of the Beale function at the given position(s)</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.Rosenbrock(ILNumerics.ILInArray{System.Single})">
      <summary>
            Rosenbrock function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x"> The position to be evaluated. it can have more than 2 two components</param>
      <returns>The evaluation of the Rosembrock function at the given position</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.Rosenbrock(ILNumerics.ILInArray{System.Double})">
      <summary>
            Rosenbrock function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x"> The position to be evaluated. it can have more than 2 two components</param>
      <returns>The evaluation of the Rosembrock function at the given position</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.Second(ILNumerics.ILInArray{System.Double})">
      <summary>
            Second function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x">Position to be evaluated. The position is in R<sup>n</sup></param>
      <returns>Evaluation of the function at a given point x or columwise evaluation of x if x is a matrix.</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.Camel3(ILNumerics.ILInArray{System.Double})">
      <summary>
            Camel function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x"> Position to be evaluated</param>
      <returns>The value of the Camel function at a given position</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.Griewank(ILNumerics.ILInArray{System.Double})">
      <summary>
            Griewank function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x"> Positin to be evaluated in dimension n </param>
      <returns>An evaluation of the Griewank function at a given position</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="M:ILNumerics.Toolboxes.Optimization.Griewank(ILNumerics.ILInArray{System.Single})">
      <summary>
            Griewank function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x">Position to be evaluated, length n</param>
      <returns>Evaluation of the Griewank function at the given position</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="P:ILNumerics.Toolboxes.Optimization.DefaultTolerance">
      <summary>
            Default tolerance for exit conditions of BFGS, L-BFGS and Newton unconstrained solvers.
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <remarks>Tolerances are expected to be small positive scalars. They are used as default exit thresholds 
            for the gradient norms and the solution offsets between subsequent iterations. Default value: 1e-8.<para>[ILNumerics Optimization Toolbox]</para></remarks>
    </member>
    <member name="T:ILNumerics.Toolboxes.Optimization.DerivativeFunction`1">
      <summary>
            Function prototype for derivative computations
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="func">Objective function</param>
      <param name="x">Input vector, current position</param>
      <param name="fx">Result of evaluation of the objective function <paramref name="func" /> at <paramref name="x" /></param>
      <returns>Jacobian matrix of the objective function at position <paramref name="x" /></returns>
      <remarks>
        <para>A derivative function is used to provide optimization algorithms with information
            about the derivative of the objective function at a certain position. Both, the Jacobian matrix and the Hessian matrix 
            can be computed using derivative functions. User defined implementations may be given to the 
            optimization algorithm. These implementations can profit from insights into the nature of the objective 
            function, which can lead to both: more exact and faster derivative determination, hence a 
            faster/better convergence of the algorithm.</para>
        <para>A user defined implementation has the option to consider any of the input parameters for the 
            jacobian estimation. An analytical implementation of some known function would not need to use the objective 
            function provided. Finite difference method mostly do.</para>
        <para>Custom implementations may not rely on the fact that <paramref name="fx" /> has been provided. However, it should 
            test for <paramref name="fx" /> being not null before evaluating the objective function at <paramref name="x" /> itself. That 
            way the number of evaluations of <paramref name="func" /> can be decreased. The overall performance is often highly influenced 
            by the number of evaluations of <paramref name="func" />.</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="T:ILNumerics.Toolboxes.Optimization.ObjectiveFunction`1">
      <summary>
            Function prototype for an objective function
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <param name="x">Input vector, length m. The current position of the optimization progress. The point in m-dim space to evaluate.</param>
      <returns>Vector of length n with the result of the evaluations of the n functions implemented by this objective function.</returns>
      <remarks>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
    </member>
    <member name="T:ILNumerics.Toolboxes.Optimization.FminIterationCallback`1">
      <summary>
            Function prototype for a callback on each iteration of <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" />
            <para>[ILNumerics Optimization Toolbox]</para>
      </summary>
      <typeparam name="T">element type, currently always: System.double</typeparam>
      <param name="info">Instance of the <see cref="T:ILNumerics.Toolboxes.FminCallbackInfo`1" /> class.</param>
      <remarks>
        <para>If user provides a function as the callback parameter of <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" />, the function gets 
            called at each major iteration. The <paramref name="info" /> parameter provides extensive information about 
            the state of the optimization process, including details of the subproblem solved in that iteration.</para>
        <para>Callback is a user defined function which gets called by <see cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})" /> and allows access 
            to the information by means of properties of info.</para>
        <para>[ILNumerics Optimization Toolbox]</para>
      </remarks>
      <seealso cref="T:ILNumerics.Toolboxes.FminCallbackInfo`1" />
      <seealso cref="M:ILNumerics.Toolboxes.Optimization.fmin(ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.ILInArray{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.Optimization.ObjectiveFunction{System.Double},ILNumerics.Toolboxes.HessianUpdateMethod,System.Int32,System.Nullable{System.Double},System.Nullable{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.ILOutArray{System.Double},ILNumerics.Toolboxes.Optimization.FminIterationCallback{System.Double})"></seealso>
    </member>
  </members>
</doc>